{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get mnist data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature & class dimensions\n",
    "n_features = 28 * 28\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_single_layer(n_features, n_classes, X, Y, learning_rate):    \n",
    "    # weights & bias for nn layers\n",
    "    W = tf.Variable(tf.random_normal([n_features, n_classes]), name='weight')\n",
    "    b = tf.Variable(tf.random_normal([n_classes]), name='bias')\n",
    "    \n",
    "    # out hypothesis\n",
    "    hypothesis = tf.matmul(X, W) + b\n",
    "    \n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return hypothesis, cost, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define multi-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multi_layer(n_features, n_classes, X, Y, learning_rate):    \n",
    "    # weights & bias for nn layers\n",
    "    W1 = tf.Variable(tf.random_normal([n_features, 256]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "    W3 = tf.Variable(tf.random_normal([256, n_classes]))\n",
    "    b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    # out hypothesis\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    \n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return hypothesis, cost, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define multi-layer with Xavier initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_multi_layer_Xavier(n_features, n_classes, X, Y, learning_rate):    \n",
    "    # weights & bias for nn layers\n",
    "    W1 = tf.get_variable(\"W1\", shape=[n_features, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "    W3 = tf.get_variable(\"W3\", shape=[256, n_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    # out hypothesis\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    \n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return hypothesis, cost, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DNN_layer_Xavier(n_features, n_classes, X, Y, learning_rate):    \n",
    "    # weights & bias for nn layers\n",
    "    W1 = tf.get_variable(\"W1\", shape=[n_features, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "    W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "    W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "    W5 = tf.get_variable(\"W5\", shape=[512, n_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "    # out hypothesis\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return hypothesis, cost, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define DNN with drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DNN_layer_Xavier_dropout(n_features, n_classes, X, Y, learning_rate, keep_prob):    \n",
    "    # weights & bias for nn layers\n",
    "    W1 = tf.get_variable(\"W1\", shape=[n_features, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "    W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "    W5 = tf.get_variable(\"W5\", shape=[512, n_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "    # out hypothesis\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return hypothesis, cost, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "lr = 0.001 # learning_rate\n",
    "kp = 0.7 # keep_prob\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select what kind of nn to use\n",
    "# hypothesis, cost, optimizer = build_single_layer(n_features, n_classes, X, Y, learning_rate) # test acc: 0.89\n",
    "# hypothesis, cost, optimizer = build_multi_layer(n_features, n_classes, X, Y, learning_rate) # test acc: 0.90\n",
    "# hypothesis, cost, optimizer = build_multi_layer_Xavier(n_features, n_classes, X, Y, learning_rate) # test acc: 0.97\n",
    "# hypothesis, cost, optimizer = build_DNN_layer_Xavier(n_features, n_classes, X, Y, learning_rate) # test acc: 0.98\n",
    "hypothesis, cost, optimizer = build_DNN_layer_Xavier_dropout(n_features, n_classes, X, Y, learning_rate, keep_prob) # test acc: 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.448471501\n",
      "Epoch: 0002 cost = 0.173111820\n",
      "Epoch: 0003 cost = 0.129690169\n",
      "Epoch: 0004 cost = 0.107324867\n",
      "Epoch: 0005 cost = 0.095596550\n",
      "Epoch: 0006 cost = 0.080282506\n",
      "Epoch: 0007 cost = 0.074212629\n",
      "Epoch: 0008 cost = 0.067507604\n",
      "Epoch: 0009 cost = 0.063255913\n",
      "Epoch: 0010 cost = 0.059632673\n",
      "Epoch: 0011 cost = 0.058608223\n",
      "Epoch: 0012 cost = 0.052385167\n",
      "Epoch: 0013 cost = 0.051235983\n",
      "Epoch: 0014 cost = 0.046335729\n",
      "Epoch: 0015 cost = 0.045655064\n",
      "Learning Finished!\n",
      "Test set accuracy: 0.9824\n",
      "Label:  [5]\n",
      "Single prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiAowg8ERfiBoP4f+6V0Uj3RlzcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129afddd908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training single layer\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # train my model\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_xs, Y: batch_ys, learning_rate: lr, keep_prob: kp}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print('Learning Finished!')\n",
    "    \n",
    "    # Test model and check accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Test set accuracy:', sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0}))\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Single prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1.0}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        #self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        self.inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        #cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        def build_cell(lstm_size, keep_prob):\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        #self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        seq_output = tf.concat(outputs, axis=1)\n",
    "        x = tf.reshape(seq_output, [-1, lstm_size])\n",
    "        \n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([lstm_size, num_classes], stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "        \n",
    "        self.logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "        self.prediction = tf.nn.softmax(self.logits, name='predictions')\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        #self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        y_one_hot = tf.one_hot(self.targets, num_classes)\n",
    "        y_reshaped = tf.reshape(y_one_hot, self.logits.get_shape())\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y_reshaped)\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "        \n",
    "        #self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.optimizer = train_op.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4202...  0.3931 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3360...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.9438...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 6.0447...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.1006...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8226...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7023...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.6441...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.5253...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4419...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3647...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3401...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3147...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3165...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.2976...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.2782...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2619...  0.1600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.2803...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2661...  0.1630 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2209...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2461...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2283...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2310...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2165...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2095...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2220...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2204...  0.1620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2005...  0.1660 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2058...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2062...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2269...  0.1582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.1930...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1798...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2002...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1795...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.1920...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1678...  0.1601 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1706...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1659...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1651...  0.1600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1556...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1677...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1537...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1544...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1459...  0.1640 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1604...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1713...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1693...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1684...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1606...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1568...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1442...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1521...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1369...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1533...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1292...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1437...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1448...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1315...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1447...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1387...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1559...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1580...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1202...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1302...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1476...  0.1581 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1394...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.0993...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1176...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1286...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1235...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1401...  0.1592 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1191...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1215...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1296...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1287...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1267...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1186...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1089...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1019...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1144...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1166...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1152...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1059...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.0891...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.0955...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.0836...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.0834...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1028...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1012...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.0942...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.0824...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.0872...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.0819...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.0655...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.0678...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.0776...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.0629...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.0679...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0530...  0.1600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.0605...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0491...  0.1581 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0437...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0365...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0436...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0257...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0227...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0145...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0337...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 2.9886...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 2.9921...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0109...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 2.9770...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 2.9613...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 2.9582...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 2.9390...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 2.9496...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 2.9571...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 2.9578...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 2.9293...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 2.9501...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 2.9200...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 2.9064...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 2.9491...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 2.9186...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 2.8613...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 2.9039...  0.1620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 2.8990...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 2.8692...  0.1600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 2.8614...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 2.8602...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.8285...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.8279...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.8202...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.7797...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.7777...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.7818...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.7886...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.7879...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.7739...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.7753...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.7283...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.7380...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.7180...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.7175...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.7267...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.6982...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.7210...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.6641...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.6773...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.7074...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.6978...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.6584...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.6567...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.6352...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.6241...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.6062...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.6064...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.5806...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.6205...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.5981...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.5606...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.5607...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.5782...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.5742...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.5609...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.5720...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.5587...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.5495...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.5345...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.5491...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.5605...  0.1680 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.5715...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.5716...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.5996...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.5380...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.5348...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.5137...  0.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.5029...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.5002...  0.1560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.5126...  0.1590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.4985...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.4994...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.5250...  0.1601 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.5407...  0.1730 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.4927...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.4771...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.4679...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.4642...  0.1582 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.4636...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.4821...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.4452...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.4659...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.4629...  0.1571 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.4459...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.4380...  0.1580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.4409...  0.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.4280...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5165...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4088...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.4300...  0.1630 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.4404...  0.1600 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.4288...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.4348...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.4324...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.4424...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.4492...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4218...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4132...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4250...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4085...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.4509...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4136...  0.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4085...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4119...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.4447...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4137...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.3832...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.3883...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4193...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4051...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.3940...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.3772...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.3805...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.3801...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.3825...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.3929...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.3808...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.3926...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.3622...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.3547...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.3760...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.3475...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.3715...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.3616...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3299...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.3302...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.3298...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.3338...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.3310...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3123...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3135...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3291...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.2911...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.3357...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3205...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3186...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.3501...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3007...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3328...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3057...  0.1690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3060...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.2981...  0.1690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3181...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3015...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.2919...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.2996...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3276...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3025...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3124...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3189...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.2982...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.2786...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3159...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.2927...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.2610...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.2531...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.2881...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3009...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.2812...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.2933...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.2600...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.2632...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3131...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.2731...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.2770...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.2445...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.2590...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.2345...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.2715...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.2330...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2235...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2177...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.2411...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.2432...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.2342...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2132...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.2539...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2190...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.2304...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2064...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2109...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2100...  0.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2178...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2120...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2133...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2054...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.1905...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.2284...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2149...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.1939...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.1942...  0.1571 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2001...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2068...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.1941...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2198...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2143...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.1925...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.1932...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2071...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.1832...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.1867...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.1761...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.1479...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.1936...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.1785...  0.1780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2055...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.1909...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2104...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.1650...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.1639...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.1971...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.1817...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.1433...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.1810...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.1845...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.1764...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.1793...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.1548...  0.1600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.1369...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.1824...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.1750...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.1578...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.1689...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.1735...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.1661...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.1942...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.1492...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.1689...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.1434...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.1411...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.1443...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.1378...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.1579...  0.1640 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.1575...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.1671...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.1439...  0.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1234...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.1416...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1721...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.1543...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.1499...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1214...  0.1591 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.1279...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1346...  0.1571 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1206...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.0898...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.1580...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.1333...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1134...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1237...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1310...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1254...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1184...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1278...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.1419...  0.1561 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1048...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.0957...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1000...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1087...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.1338...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1273...  0.1590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1235...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1143...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.0958...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.0945...  0.1572 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.0732...  0.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.0549...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.0770...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.0949...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.0881...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1179...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1006...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.0792...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.0848...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.0631...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.0743...  0.1581 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.0861...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.0868...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.0480...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.0867...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.0665...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.0418...  0.1580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.0711...  0.1570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.0715...  0.1560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.0477...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.1540...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.0595...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.0469...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0631...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.0625...  0.1721 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0444...  0.1571 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.0587...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.0587...  0.1700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.0940...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.0606...  0.1571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.0387...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.0297...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.0574...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.0918...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.0554...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0264...  0.1571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.0401...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.0838...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.0504...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.0453...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.0259...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.0848...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.0392...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0331...  0.1561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.0385...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0147...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0131...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.0423...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.0661...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.0479...  0.1581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.0298...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0105...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0322...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.0633...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0200...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0230...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0148...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 1.9784...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 1.9903...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 1.9888...  0.1571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 1.9934...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0181...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0010...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 1.9856...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0145...  0.1591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 1.9568...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0193...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 1.9886...  0.1591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 1.9963...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.0432...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 1.9656...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.0462...  0.1700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 1.9925...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 1.9926...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 1.9836...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0029...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0025...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 1.9905...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 1.9749...  0.1611 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0307...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 1.9949...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0333...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0227...  0.1602 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0035...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 1.9845...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0237...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0030...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 1.9663...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 1.9666...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 1.9873...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0107...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 1.9831...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0020...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 1.9636...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 1.9722...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0054...  0.1591 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 1.9704...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 1.9785...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 1.9407...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 1.9595...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 1.9357...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 1.9773...  0.1561 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 1.9343...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 1.9581...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9276...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 1.9437...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 1.9470...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 1.9446...  0.1571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9205...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 1.9613...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9395...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 1.9481...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9238...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9344...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9227...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 1.9462...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 1.9512...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9266...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9242...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9084...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 1.9507...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 1.9413...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9325...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9385...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9162...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9452...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9369...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 1.9572...  0.1581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 1.9502...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 1.9453...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9266...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9321...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9275...  0.1610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9185...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9207...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.8914...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9225...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9153...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9260...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9323...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 1.9517...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9005...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9160...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 1.9492...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9268...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.8793...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9404...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9358...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9167...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9129...  0.1720 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.8911...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.8918...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9318...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9109...  0.1571 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9149...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9201...  0.1581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9257...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9192...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 1.9509...  0.1721 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9048...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9319...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.8984...  0.1680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9069...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9110...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.8860...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9117...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9238...  0.1750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9304...  0.1610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9039...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.8877...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9012...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9347...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9158...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9107...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.8963...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.8939...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9099...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.8889...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.8642...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9313...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9194...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.8940...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9091...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9001...  0.1700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.8859...  0.1610 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.8897...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9123...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9403...  0.1601 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.8706...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.8851...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.8745...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.8798...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9098...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.8989...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.8892...  0.1600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.8782...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.8694...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.8920...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.8590...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8435...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.8620...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.8771...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.8749...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.8952...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.8743...  0.1581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.8634...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.8730...  0.1590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.8431...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.8598...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.8773...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.8821...  0.1710 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8368...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.8769...  0.1580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.8478...  0.1630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8309...  0.1581 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.8656...  0.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.8634...  0.1570 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.8465...  0.1740 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9492...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.8506...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8537...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.8645...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.8577...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8225...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.8654...  0.1680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8424...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.8813...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.8501...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8323...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8321...  0.1572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8611...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.8978...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8452...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8191...  0.1571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8417...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.8773...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8457...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.8496...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8399...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.8743...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8423...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8458...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8451...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8133...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8179...  0.1650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.8455...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.8736...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.8513...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8407...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8184...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8522...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.8680...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8193...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8377...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8192...  0.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.7991...  0.1591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.7857...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.7955...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8062...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8422...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8063...  0.1700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.7944...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8309...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.7809...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8118...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.7962...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8069...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8516...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.7809...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.8635...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8161...  0.1690 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8155...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.7965...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8162...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8240...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8005...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.7980...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8466...  0.1571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8280...  0.1561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.8447...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8526...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8319...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8029...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8339...  0.1591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8363...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.7938...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.7973...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8026...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8509...  0.1571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8175...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8332...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.7916...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8019...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8342...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8066...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8103...  0.1591 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.7761...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.7943...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.7538...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8252...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.7632...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.7924...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.7668...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.7760...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.7782...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.7728...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.7574...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8134...  0.1601 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.7597...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.7822...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.7648...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.7654...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.7740...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.7906...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.7839...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.7702...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.7654...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7364...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.7998...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.7769...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.7715...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.7672...  0.1572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.7609...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7794...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.7733...  0.1610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.7849...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.7832...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.7850...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.7795...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.7661...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.7649...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.7646...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7527...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7469...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.7888...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.7649...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.7754...  0.1572 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.7663...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.7786...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7337...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7445...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.7884...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7595...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7265...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.7883...  0.1640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.7838...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7613...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7543...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7377...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7362...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.7800...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.7671...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.7623...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.7611...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.7803...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.7718...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.7924...  0.1670 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.7524...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8017...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7517...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7512...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.7619...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7538...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.7686...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.7729...  0.1581 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.7876...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.7638...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7439...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7217...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.7734...  0.1561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.7706...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.7583...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7464...  0.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7448...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.7658...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7422...  0.1561 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7192...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.7688...  0.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.7654...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7492...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.7653...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7501...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7455...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7337...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7548...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8026...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7343...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7370...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7373...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7212...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.7609...  0.1660 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7583...  0.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7546...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7270...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7175...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7593...  0.1571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7124...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7086...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7126...  0.1610 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7377...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7284...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7470...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7273...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7131...  0.1571 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7450...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7130...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7220...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7328...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7310...  0.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7011...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7189...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7108...  0.1560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.6958...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7250...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7242...  0.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.6974...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8105...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7216...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7044...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7230...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7110...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.6708...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7033...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.6915...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7371...  0.1640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7172...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7017...  0.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7112...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7230...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7646...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7130...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.6973...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7149...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7443...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7220...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7333...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.6933...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7352...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7037...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7166...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7184...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.6743...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.6853...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7151...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7403...  0.1611 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7286...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7110...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.6909...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7140...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7302...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.6947...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7071...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.6864...  0.1582 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.6721...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.6569...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.6785...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.6814...  0.1561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7193...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.6736...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.6670...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7146...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6494...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.6883...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.6835...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.6771...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7355...  0.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.6642...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7546...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.6797...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.6861...  0.1690 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.6806...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7058...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7078...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6768...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.6711...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7186...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.6834...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7346...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7221...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7009...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.6867...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7058...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7088...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.6719...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.6854...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.6731...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7204...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.6891...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7240...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.6697...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.6719...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7058...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.6811...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.6865...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6562...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.6760...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6368...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.6949...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6414...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.6787...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6433...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6683...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6486...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6537...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6397...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.6859...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6427...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.6510...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6447...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6454...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6460...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.6715...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.6577...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6270...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6583...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6315...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.6665...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6611...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6575...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6504...  0.1561 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6563...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.6530...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.6657...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6736...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.6618...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.6802...  0.1581 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6555...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6498...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6620...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6563...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6371...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6170...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.6512...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6513...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6474...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6555...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.6556...  0.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6210...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6128...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.6748...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6491...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6042...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.6683...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.6632...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6389...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6293...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6146...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6220...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.6639...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6666...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6605...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6539...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.6687...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6690...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.6576...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6415...  0.1571 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7048...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6469...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6444...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.6643...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6382...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.6656...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6555...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.6903...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6601...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6375...  0.1690 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6087...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6612...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6555...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6499...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6322...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6387...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6640...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6333...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6035...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.6579...  0.1560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.6742...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6413...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6528...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6397...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6329...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6351...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.6589...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7109...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6329...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6356...  0.1640 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6310...  0.1610 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6166...  0.1600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.6530...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6509...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6589...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6243...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6191...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6564...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6116...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6083...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.5932...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6181...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6229...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6436...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6344...  0.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6189...  0.1570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6580...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6221...  0.1590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6255...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6354...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6287...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6136...  0.1581 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6397...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6054...  0.1600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6022...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6247...  0.1580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6262...  0.1600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6054...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7139...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6184...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6126...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6354...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6071...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.5846...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6254...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.5986...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6395...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6248...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.5897...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6048...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6330...  0.1611 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6702...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6157...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6043...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6227...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6511...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6247...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6445...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6179...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6515...  0.1581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6147...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6413...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6182...  0.1620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.5881...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.5834...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6248...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6379...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6316...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6192...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.5933...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6307...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6256...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6038...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6107...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.5803...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.5781...  0.1610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.5656...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.5937...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.5878...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6384...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.5785...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.5788...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6189...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5585...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.5914...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.5840...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.5911...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6348...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.5844...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6566...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6025...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6054...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.5909...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6080...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6176...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.5848...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.5821...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6318...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6028...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6427...  0.1572 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6414...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6109...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.5959...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6095...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6103...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.5838...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6109...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.5826...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6417...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6203...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6243...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.5823...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.5817...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6138...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.5854...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.5926...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5467...  0.1591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.5918...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5480...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.5978...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5552...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.5882...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5661...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.5718...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5603...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5727...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5526...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6008...  0.1640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5575...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.5768...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5669...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5646...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5656...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.5850...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.5801...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5544...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.5655...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5464...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.5838...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.5749...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.5854...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.5740...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.5666...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.5714...  0.1591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.5796...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.5812...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.5827...  0.1630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6033...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.5686...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.5794...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.5690...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.5698...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5540...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5365...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.5726...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.5829...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.5712...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.5715...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.5824...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5500...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5405...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.5913...  0.1581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.5711...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5268...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.5865...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.5851...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5688...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5441...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5239...  0.1610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5419...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.5873...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.5804...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.5867...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.5636...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.5946...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.5857...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.5781...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.5650...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6268...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.5719...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.5727...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6093...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.5672...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.5972...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.5929...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6260...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.5911...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5706...  0.1600 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5449...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.5800...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.5874...  0.1610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.5832...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.5751...  0.1610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5770...  0.1571 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.5897...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.5747...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5393...  0.1591 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5958...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6015...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.5706...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.5803...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.5669...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5645...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.5688...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.5903...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6354...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5669...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5508...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5685...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5491...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.5883...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.5730...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.5759...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5382...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5457...  0.1630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.5832...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5478...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5309...  0.1610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5278...  0.1581 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5489...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5594...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.5568...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5582...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5466...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.5761...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5458...  0.1561 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5536...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5672...  0.1560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5392...  0.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5288...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5468...  0.1580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5340...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5201...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5608...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5459...  0.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5375...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6419...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5513...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5378...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.5697...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5223...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5094...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5453...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5314...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5630...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5483...  0.1601 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5268...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5419...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5455...  0.1620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.5780...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5319...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5213...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5530...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.5668...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5451...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5774...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5411...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.5627...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5493...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5583...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5530...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5026...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5100...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5601...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5668...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5700...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5448...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5139...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5543...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5455...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5317...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5396...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5132...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5029...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.4898...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5099...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5155...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.5701...  0.1660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5159...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5045...  0.1701 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5544...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.4994...  0.1572 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5261...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5175...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5272...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5595...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5146...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.5825...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5325...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5341...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5325...  0.1650 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5395...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5644...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5194...  0.1572 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5196...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5561...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5390...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5785...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5635...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5491...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5318...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5496...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5463...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5125...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5310...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5231...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.5707...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5468...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5673...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5187...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5242...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5607...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5239...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5113...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.4812...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5275...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.4793...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5399...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.4843...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5118...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.4887...  0.1630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.4990...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.4959...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.4886...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.4830...  0.1591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5276...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.4917...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.4911...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.4950...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.4993...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.4955...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5240...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5101...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.4786...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.4928...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.4740...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5159...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5194...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5177...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5197...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5005...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5134...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5197...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5243...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5098...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5318...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.4955...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5197...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5063...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5187...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.4945...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.4771...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5213...  0.1561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5137...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5091...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.4975...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5133...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.4814...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.4646...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5108...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5040...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.4668...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5148...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5127...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.4952...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.4650...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.4633...  0.1572 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.4731...  0.1590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5264...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5079...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5109...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5046...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5331...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5085...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5029...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.4918...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5580...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5082...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.4999...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5306...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.4756...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5196...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5025...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5306...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5183...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.4879...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4639...  0.1582 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.4965...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5156...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.4939...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5051...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5036...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5181...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.4955...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4570...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5206...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5211...  0.1620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.4947...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.4975...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.4860...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.4889...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5040...  0.1571 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5235...  0.1610 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.5618...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.4928...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.4936...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.4842...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.4711...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5131...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5055...  0.1561 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5155...  0.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.4683...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.4682...  0.1700 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5225...  0.1581 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.4602...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4589...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.4667...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.4824...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.4788...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.4953...  0.1591 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.4770...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.4842...  0.1682 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5100...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.4818...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.4890...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.4941...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.4816...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.4625...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.4796...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.4661...  0.1580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4464...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.4862...  0.1570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.4778...  0.1560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.4673...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.5897...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.4930...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.4896...  0.1581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5111...  0.1591 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.4639...  0.1561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4407...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.4826...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.4682...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5010...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.4699...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4565...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.4558...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.4708...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5046...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.4672...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4552...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.4888...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5047...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.4765...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5081...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.4824...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.4961...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.4746...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.4919...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.4801...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4394...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4484...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.4875...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.4930...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.4972...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.4617...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4490...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.4917...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.4762...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.4702...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.4685...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.4539...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4380...  0.1572 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4189...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.4543...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.4501...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5015...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4477...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4363...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.4704...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4269...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4566...  0.1581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4473...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.4661...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.4872...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4454...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5159...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.4648...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.4652...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.4559...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.4717...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.4896...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4530...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4362...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5042...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.4781...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5212...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.4926...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4748...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.4646...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.4793...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.4793...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4509...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.4734...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4627...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5088...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.4899...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5069...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4381...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.4591...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.4835...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4583...  0.1581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4513...  0.1561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4241...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.4724...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4110...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.4674...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4297...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4541...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4381...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4519...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4390...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4312...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4272...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.4712...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4311...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4415...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4389...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4306...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4341...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4604...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4594...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4308...  0.1561 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4319...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4163...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4589...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4447...  0.1571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4510...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4488...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4404...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4448...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4487...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4465...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4518...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.4802...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4510...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4621...  0.1581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4425...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4366...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4363...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4158...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4524...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4692...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4572...  0.1581 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4447...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4506...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4261...  0.1950 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4156...  0.1670 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4586...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4454...  0.1640 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4030...  0.1740 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4703...  0.1720 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4657...  0.1730 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4477...  0.1700 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4168...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4056...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4271...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.4699...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4544...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4568...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4660...  0.1690 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.4794...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.4669...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.4612...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4574...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5035...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4607...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4374...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.4807...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4323...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.4712...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.4463...  0.1650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.4852...  0.1670 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.4824...  0.1710 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4404...  0.1660 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4195...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4381...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4539...  0.1730 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4378...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4395...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4486...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4552...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4440...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4138...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.4612...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.4708...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4432...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4457...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4483...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4419...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4477...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4588...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5135...  0.1560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4424...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4400...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4324...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4260...  0.1620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.4706...  0.1640 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4417...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.4510...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4151...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4280...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4698...  0.1630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4188...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4038...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4139...  0.1590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4271...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4380...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4382...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4296...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4232...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.4579...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4260...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4280...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4379...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4211...  0.1610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4190...  0.1550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4281...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4098...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4030...  0.1580 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4326...  0.1570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4320...  0.1600 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4159...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.5550...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4633...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4391...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4592...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4272...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4079...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4404...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4330...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4431...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4284...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4200...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4188...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4328...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4613...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4266...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4184...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4329...  0.1562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4479...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4426...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4633...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4287...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4496...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4274...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4331...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4360...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.3923...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.3981...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4411...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4400...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4513...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4187...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.3949...  0.1550 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4387...  0.1630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4346...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4205...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4337...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.3999...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.3801...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.3782...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4056...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4002...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4444...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4024...  0.1670 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4014...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4299...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.3851...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4258...  0.1660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4098...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4270...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4316...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4043...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4645...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4246...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4351...  0.1630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4217...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4215...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4416...  0.1650 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4084...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4002...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4508...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4316...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.4695...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4418...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4405...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4249...  0.1660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4395...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4337...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4139...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4324...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4030...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4565...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4432...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4581...  0.1660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4043...  0.1581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4195...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4397...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4143...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4082...  0.1581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.3723...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4193...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.3782...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4253...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.3814...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4103...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.4017...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4069...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.3972...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4010...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.3784...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4279...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.3941...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4030...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.3901...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.3906...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.3920...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4147...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4176...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.3735...  0.1581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.3825...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.3805...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4097...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4055...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4093...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4097...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4003...  0.1621 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4041...  0.1630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4059...  0.1581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4141...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.3950...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4241...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.3936...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4118...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4106...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.3978...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.3910...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3630...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4072...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4116...  0.1600 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4080...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.3971...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4156...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.3759...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3554...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4143...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4001...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.3624...  0.1571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4164...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4152...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.3915...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.3744...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3598...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.3931...  0.1582 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4294...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4066...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4113...  0.1581 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4156...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4338...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4265...  0.1560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4140...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4159...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4697...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4114...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4055...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4301...  0.1571 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.3825...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4213...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4170...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4423...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4396...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4034...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3731...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.3854...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4155...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4002...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4031...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4042...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4182...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.3985...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3675...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4160...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4357...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4028...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4024...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.3986...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4107...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4035...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4254...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.4674...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4096...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.3970...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.3905...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.3834...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4362...  0.1590 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4012...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4187...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.3758...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.3887...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4294...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.3792...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3662...  0.1600 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.3788...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.3971...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4006...  0.1630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.3970...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.3962...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.3869...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4240...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.3982...  0.1640 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.4010...  0.1650 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.3988...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.3811...  0.1620 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3797...  0.1610 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.3984...  0.1601 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.3748...  0.1570 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3591...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.3951...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.3993...  0.1580 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.3812...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5200...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4031...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4006...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4190...  0.1571 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.3885...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3668...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.3984...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.3891...  0.1620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4133...  0.1592 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.3989...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.3729...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.3886...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.3976...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4207...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.3910...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.3766...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4142...  0.1620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4156...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.3977...  0.1561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4167...  0.1561 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.3905...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4028...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.3907...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4089...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4129...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3452...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3631...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4092...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.3994...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4048...  0.1630 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.3824...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3758...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4062...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4051...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.3864...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.3984...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3689...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3505...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3467...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3666...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3601...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4296...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3770...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3612...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4123...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3551...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.3842...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.3788...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.3890...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4031...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3595...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4274...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.3895...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.3958...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.3804...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.3958...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4086...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3867...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3637...  0.1750 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4228...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.3938...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4338...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4103...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4075...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.3819...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.3913...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.3925...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3709...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.3882...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.3769...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4293...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4092...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4280...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3669...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.3810...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4048...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.3778...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.3695...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3369...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.3861...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3394...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.3817...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3486...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.3788...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3637...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3808...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3478...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3609...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3523...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.3912...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3586...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3751...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3636...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3543...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3566...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.3878...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.3892...  0.1680 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3428...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3586...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3577...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3778...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.3735...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.3708...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3734...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3713...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.3782...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.3802...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.3775...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3617...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.3862...  0.1600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3576...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.3694...  0.1640 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.3733...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3710...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3562...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3266...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.3805...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3650...  0.1560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3723...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3790...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3714...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3390...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3167...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.3787...  0.1571 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3704...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3238...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.3831...  0.1600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.3823...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3619...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3286...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3261...  0.1720 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3614...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.3975...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.3860...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.3851...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3785...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.3897...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.3903...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.3829...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.3740...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4268...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.3863...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3750...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4095...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3498...  0.1600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.3911...  0.1640 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.3835...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4118...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.3962...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3686...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3417...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3575...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.3876...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3701...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3694...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3717...  0.1571 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3784...  0.1600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3694...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3383...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.3957...  0.1620 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.3962...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3712...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.3722...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.3784...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.3723...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3682...  0.1560 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.3974...  0.1640 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4354...  0.1561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.3707...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3688...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3670...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3451...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4041...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3774...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.3834...  0.1600 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3476...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3643...  0.1581 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.3957...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3524...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3312...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3568...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3644...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3712...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3633...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3631...  0.1610 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3533...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.3909...  0.1592 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3634...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3563...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3674...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3548...  0.1570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3448...  0.1571 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3695...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3536...  0.1590 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3357...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3754...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3569...  0.1580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3477...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.4945...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.3793...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3672...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.3834...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3491...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3361...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.3668...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3568...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.3782...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3625...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3511...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3648...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3666...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.3888...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3529...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3365...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.3863...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.3902...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3727...  0.1582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.3937...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3684...  0.1620 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.3712...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3549...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.3779...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3701...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3230...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3302...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.3859...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.3734...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.3738...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3549...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3303...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3777...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3683...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3516...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3588...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3381...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3184...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3077...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3492...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3334...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.3869...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3388...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3302...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3677...  0.1578 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3309...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3408...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3481...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3523...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3684...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3270...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.3976...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3581...  0.1601 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.3725...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3593...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3626...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.3750...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3491...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3355...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.3994...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3653...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4062...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.3845...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3704...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3526...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3732...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3712...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3347...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3676...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3455...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4006...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.3795...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.3946...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3409...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3539...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3742...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3648...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3480...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3125...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3707...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3206...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3554...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3196...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3376...  0.1571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3365...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3479...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3293...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3337...  0.1572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3113...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3614...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3318...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3430...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3422...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3244...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3406...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3610...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3589...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3137...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3333...  0.1572 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3305...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3590...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3415...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3482...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3543...  0.1720 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3337...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3557...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3508...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3576...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3428...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3593...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3376...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3511...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3498...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3432...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3281...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3101...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3467...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3493...  0.1571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3439...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3413...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3431...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3253...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3024...  0.1650 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3607...  0.1591 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3343...  0.1681 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3003...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3498...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3518...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3300...  0.1582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3060...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3037...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3379...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3694...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3540...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3646...  0.1591 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3560...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.3743...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3624...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3530...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3558...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4071...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3657...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3509...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.3857...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3384...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3683...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3561...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.3922...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3805...  0.1571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3414...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3179...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3350...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3665...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3393...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3358...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3480...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3560...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3425...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3189...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3700...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.3724...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3486...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3430...  0.1571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3424...  0.1620 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3516...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3455...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3603...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4103...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3540...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3463...  0.1560 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3394...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3211...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3662...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3493...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3551...  0.1600 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3232...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3431...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3783...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3214...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3132...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3188...  0.1611 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3504...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3462...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3365...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3268...  0.1591 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3338...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3772...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3365...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3431...  0.1590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3424...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3225...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3198...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3436...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3181...  0.1570 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3048...  0.1610 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3464...  0.1580 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3342...  0.1581 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3299...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.4655...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3419...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3464...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3668...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3279...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3120...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3430...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3406...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3497...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3441...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3137...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3382...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3361...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3532...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3223...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3210...  0.1581 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3588...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3586...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3351...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3682...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3398...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3559...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3319...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3503...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3346...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.2904...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3073...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3566...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3501...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3512...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3303...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3103...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3526...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3510...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3207...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3425...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3098...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.2886...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.2920...  0.1581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3212...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3184...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3701...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3253...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3041...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3556...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3114...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3148...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3263...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3318...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3536...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3044...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.3841...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3342...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3447...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3366...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3327...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3495...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3248...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3164...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.3679...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3364...  0.1581 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.3867...  0.1721 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3542...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3524...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3338...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3518...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3447...  0.1572 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3228...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3388...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3145...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.3717...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3489...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3768...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3167...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3362...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3523...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3261...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3222...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.2875...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3412...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.2894...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3335...  0.1610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.3015...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3232...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3144...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3307...  0.1630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3061...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3126...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.2913...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3349...  0.1620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3026...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3195...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3087...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3057...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3104...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3443...  0.1610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3415...  0.1650 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3007...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3133...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3069...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3366...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3229...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3266...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3193...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3087...  0.1620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3263...  0.1610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3424...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3281...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3069...  0.1620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3443...  0.1610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3156...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3293...  0.1620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3294...  0.1760 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3195...  0.1590 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.2980...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.2879...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3266...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3289...  0.1600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3329...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3274...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3351...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2872...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.2769...  0.1598 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3349...  0.1576 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3203...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.2764...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3400...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3348...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3104...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.2849...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.2791...  0.1561 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3145...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3440...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3299...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3368...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3259...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3501...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3449...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3361...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3304...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.3865...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3434...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3260...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3548...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3014...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3469...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3251...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3536...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3557...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3145...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.3037...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3003...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3470...  0.1710 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3073...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3128...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3196...  0.1643 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3346...  0.1630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3224...  0.1595 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.2981...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3440...  0.1612 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3579...  0.1650 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3318...  0.1585 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3281...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3292...  0.1588 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3314...  0.1621 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3160...  0.1592 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3477...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.3923...  0.1652 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3202...  0.1590 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3260...  0.1584 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3075...  0.1615 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3005...  0.1592 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3463...  0.1597 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3182...  0.1627 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3293...  0.1591 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.2947...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3180...  0.1611 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3493...  0.1605 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.2973...  0.1601 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.2947...  0.1612 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.2963...  0.1577 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3235...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3144...  0.1607 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3196...  0.1610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3036...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3007...  0.1560 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3504...  0.1595 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3236...  0.1605 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3063...  0.1642 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3158...  0.1575 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.2997...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3059...  0.1570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3227...  0.1580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.2922...  0.1602 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.2750...  0.1624 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3198...  0.1571 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3209...  0.1720 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3043...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4475...  0.1626 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3263...  0.1565 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3163...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3405...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3171...  0.1592 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.2860...  0.1607 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3190...  0.1577 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3100...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3193...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3134...  0.1609 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3070...  0.1625 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3068...  0.1582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3193...  0.1608 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3304...  0.1600 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.2953...  0.1623 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.2906...  0.1601 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3346...  0.1572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3394...  0.1581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3108...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3446...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3042...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3345...  0.1571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3053...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3312...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3176...  0.1640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.2732...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.2829...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3364...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3326...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3261...  0.1571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3001...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.2871...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3251...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3260...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.2961...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3189...  0.1604 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.2934...  0.1607 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2706...  0.1586 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2751...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3022...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.2961...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3543...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3087...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.2815...  0.1630 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3255...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.2856...  0.1607 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3017...  0.1603 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.2979...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3094...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3246...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.2844...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3532...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3172...  0.1611 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3194...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3039...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3094...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3252...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3021...  0.1614 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.2915...  0.1635 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3610...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3148...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3494...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3269...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3214...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3141...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3276...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3300...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.2983...  0.1582 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3147...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.2941...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3491...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3258...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3553...  0.1640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.2896...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3092...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3303...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3062...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3007...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.2638...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3169...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2712...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3072...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.2742...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3076...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.2924...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3069...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.2816...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.2841...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.2815...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3180...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.2899...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.2946...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.2902...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.2825...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.2943...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3152...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3173...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.2728...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.2930...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.2815...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3165...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.2999...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.2968...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3108...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.2905...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3006...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3127...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3183...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.2809...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3195...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.2926...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3041...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3088...  0.1572 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.2964...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.2870...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2689...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3139...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3033...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3028...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3003...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3003...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2749...  0.1581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2562...  0.1581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3031...  0.1640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.2989...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2620...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3184...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3105...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.2806...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2648...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2628...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.2820...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3205...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3134...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3192...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3063...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3336...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3240...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3095...  0.1571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3017...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3546...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3188...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.2979...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3477...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.2856...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3305...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3060...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3388...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3322...  0.1603 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.2902...  0.1757 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2812...  0.1634 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.2820...  0.1650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3194...  0.1604 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.2924...  0.1585 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3005...  0.1620 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.2957...  0.1642 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3031...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3013...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2783...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3220...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3337...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3142...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3008...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3026...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3065...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.2971...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3317...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3716...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3185...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3103...  0.1571 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.2988...  0.1560 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.2830...  0.1637 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3349...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3024...  0.1581 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3079...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.2876...  0.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.3014...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3436...  0.1640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.2812...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2817...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.2870...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3133...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.2945...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.2903...  0.1570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.2991...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.2852...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3349...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.2990...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.2975...  0.1580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.3025...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2772...  0.1600 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.2809...  0.1590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3057...  0.1650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.2847...  0.1912 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2680...  0.1575 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.2999...  0.1594 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.2959...  0.1650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.2926...  0.1606 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4297...  0.1577 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3108...  0.1571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.2989...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3169...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.2825...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2644...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3015...  0.1586 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.2939...  0.1647 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3031...  0.1621 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.2857...  0.1585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.2913...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.2979...  0.1600 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3124...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3064...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.2853...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2670...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3138...  0.1640 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3243...  0.1620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.2972...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3132...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.2892...  0.1640 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3195...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.2937...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3202...  0.1630 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3084...  0.1585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2638...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2766...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3160...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3116...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3187...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.2859...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2581...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3051...  0.1582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3026...  0.1581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2756...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3114...  0.1581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.2696...  0.1597 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2625...  0.1592 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2450...  0.1615 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.2932...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2811...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3360...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.2863...  0.1680 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2695...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3020...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2709...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.2768...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.2903...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.2971...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3119...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2696...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3327...  0.1660 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.2985...  0.1578 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3105...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.2876...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.2931...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3016...  0.1612 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.2897...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2743...  0.1582 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3264...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.2918...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3405...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3204...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3061...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.2944...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3080...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3123...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.2866...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3011...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2879...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3445...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3108...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3462...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2757...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.2873...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3087...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.2953...  0.1640 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.2776...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2533...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.2998...  0.1646 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2537...  0.1581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.2962...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2618...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.2856...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2709...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.2928...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2707...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2770...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2591...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.2967...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2724...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.2753...  0.1603 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2693...  0.1585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2666...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2662...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3019...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.2954...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2620...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2699...  0.1606 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2659...  0.1607 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.2928...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.2761...  0.1566 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.2916...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.2889...  0.1571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.2850...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.2813...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.2930...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3111...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2906...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.2959...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2840...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.2876...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.2846...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.2812...  0.1561 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2698...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2559...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.2920...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.2910...  0.1571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.2861...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.2838...  0.1571 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.2968...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2629...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2428...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.2907...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2754...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2520...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.2910...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.2835...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2705...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2516...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2431...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2654...  0.1630 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3067...  0.1621 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.2915...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.3037...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.2871...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3058...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.2991...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.2946...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.2859...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3444...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3019...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2769...  0.1622 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3173...  0.1620 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2732...  0.1616 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3148...  0.1585 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.2958...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3236...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3169...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2821...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2607...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2649...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3019...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.2693...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.2850...  0.1610 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.2748...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.2868...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2815...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2576...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3052...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3101...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.2976...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2860...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2908...  0.1720 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.2731...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.2839...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3068...  0.1590 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3515...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.2987...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.2842...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.2839...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2650...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3177...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.2896...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.2868...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2652...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2827...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3135...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2561...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2554...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2716...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2728...  0.1581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.2833...  0.1581 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2681...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.2718...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2773...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3172...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2860...  0.1560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2805...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2831...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2589...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2653...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2975...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2711...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2539...  0.1570 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.2900...  0.1600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2760...  0.1580 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2766...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4084...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.2941...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.2912...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3133...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2604...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2405...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.2864...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2844...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.2935...  0.1581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2759...  0.1590 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2785...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2855...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.2788...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.3017...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2700...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2600...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3010...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.2984...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.2865...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3058...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.2895...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.2990...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2760...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.2904...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.2853...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2404...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2568...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3002...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.2929...  0.1780 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3021...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2672...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2610...  0.1561 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.2939...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.2884...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2576...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.2894...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2581...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2462...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2438...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2653...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2558...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3211...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2760...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2490...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.2883...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2596...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2700...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2781...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2812...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.2923...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2650...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3254...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.2908...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.2813...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2772...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.2755...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.2943...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2723...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2655...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3184...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.2890...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3196...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3040...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.2909...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.2748...  0.1610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.3022...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3037...  0.1700 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2682...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2852...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2751...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3276...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3033...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3155...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2590...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.2788...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.2979...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2753...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2629...  0.1620 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2308...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.2889...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2440...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.2831...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2514...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2726...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2533...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2751...  0.1720 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2579...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2560...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2394...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.2836...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2542...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2580...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2506...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2499...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2610...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.2879...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2830...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2394...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2672...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2590...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2767...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2716...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2717...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2635...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2645...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2762...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2731...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2803...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2585...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.2898...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2651...  0.1730 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.2732...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2749...  0.1581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2688...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2566...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2283...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2796...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.2780...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2765...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2745...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2754...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2325...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2278...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2696...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2610...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2223...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2744...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2793...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2487...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2287...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2236...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2526...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.2895...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.2830...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.2871...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2756...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.2966...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.2916...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.2796...  0.1610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.2764...  0.1820 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3242...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.2819...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2718...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3141...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2618...  0.1650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3014...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2784...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3073...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3065...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2672...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2568...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2547...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.2908...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2665...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2663...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2677...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2751...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2716...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2480...  0.1610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.2908...  0.1581 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.2949...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2822...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2829...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2807...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2795...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2717...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.2904...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3334...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2785...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2738...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2660...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2514...  0.1720 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.2911...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2743...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.2824...  0.1610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2447...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2570...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3077...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2541...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2452...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2545...  0.1590 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2667...  0.1571 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2714...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2631...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2630...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2603...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3013...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2688...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2634...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2657...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2468...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2511...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2724...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2590...  0.1560 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2359...  0.1570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2616...  0.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2588...  0.1580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2596...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.3900...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.2825...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2781...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.2987...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2517...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2297...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2715...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2615...  0.1660 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2705...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2675...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2581...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2633...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2677...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2822...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2533...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2489...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2816...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.2836...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2666...  0.1582 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.2868...  0.1571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2651...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.2737...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2646...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.2873...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2649...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2244...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2306...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.2857...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.2829...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2833...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2496...  0.1650 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2340...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2796...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2701...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2657...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2759...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2376...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2251...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2161...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2511...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2458...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3004...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2563...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2432...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2779...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2407...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2473...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2544...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2640...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.2855...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2357...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3091...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2738...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2786...  0.1571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2576...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2651...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2847...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2589...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2442...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.2906...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2736...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3128...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.2875...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2814...  0.1561 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2564...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2841...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.2828...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2564...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2626...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2534...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3034...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.2942...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.2999...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2465...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2691...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.2847...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2627...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2515...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2169...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2668...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2338...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2640...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2225...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2596...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2416...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2532...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2402...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2439...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2286...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2683...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2364...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2549...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2282...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2326...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2476...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2755...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2725...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2304...  0.1571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2488...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2394...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2667...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2452...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2689...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2609...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2546...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2504...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2590...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2657...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2451...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2681...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2482...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2662...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2590...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2448...  0.1571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2413...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2198...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2643...  0.1571 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2600...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2588...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2628...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2577...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2258...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2070...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2532...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2520...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2236...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2697...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2604...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2402...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2139...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2037...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2397...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2747...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2670...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2658...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2613...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.2890...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2673...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2635...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2626...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3138...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2749...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2503...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.2946...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2445...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.2802...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2665...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.2942...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.2862...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2559...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2302...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2374...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2636...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2514...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2385...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2548...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2674...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2541...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2219...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2782...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2729...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2504...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2522...  0.1640 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2601...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2560...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2616...  0.1600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.2759...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3212...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2646...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2638...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2592...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2478...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.2913...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2567...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2605...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2350...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2627...  0.1640 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.2810...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2477...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2265...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2401...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2549...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2538...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2564...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2564...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2468...  0.1590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2750...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2637...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2519...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2478...  0.1572 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2302...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2414...  0.1560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2636...  0.1570 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2373...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2247...  0.1581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2593...  0.1620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2393...  0.1580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2385...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.3738...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2698...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2519...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.2830...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2321...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2225...  0.1710 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2560...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2470...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2557...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2443...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2435...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2546...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2576...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2653...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2447...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2296...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2695...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2822...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2498...  0.1581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2885...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2562...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2645...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2461...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2786...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2504...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2188...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2303...  0.1571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2686...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2658...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2684...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2387...  0.1670 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2222...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2681...  0.1690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2593...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2485...  0.1610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2653...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2298...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2162...  0.1561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2097...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2337...  0.1582 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2288...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.2981...  0.1700 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2409...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2232...  0.1571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2643...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2268...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2430...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2453...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2539...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2618...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2273...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.2966...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2621...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2653...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2445...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2557...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2602...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2425...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2425...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.2847...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2598...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.2941...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2724...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2582...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2500...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2731...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2707...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2445...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2560...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2468...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.2976...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2765...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2852...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2348...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2571...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2677...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2537...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2433...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2049...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2608...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2140...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2501...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2200...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2502...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2321...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2499...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2280...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2368...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2220...  0.1610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2559...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2206...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2399...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2192...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2205...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2349...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2613...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2524...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2116...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2300...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2200...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2555...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2359...  0.1610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2564...  0.1571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2343...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2498...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2396...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2456...  0.1581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2625...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2314...  0.1620 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2590...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2351...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2532...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2466...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2410...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2241...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2097...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2545...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2585...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2463...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2522...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2489...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2153...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2116...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2423...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2325...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2038...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2507...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2541...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2279...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.1982...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.1970...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2246...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2673...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2525...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2569...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2498...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2803...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2609...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2598...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2474...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3008...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2601...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2452...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2796...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2314...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2743...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2603...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.2801...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2753...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2403...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2228...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2175...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2657...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2451...  0.1581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2385...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2339...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2562...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2356...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2126...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2642...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2695...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2471...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2379...  0.1620 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2460...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2500...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2437...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2711...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.2985...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2557...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2567...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2438...  0.1561 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2401...  0.1581 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2802...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2437...  0.1560 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2534...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2216...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2422...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2818...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2420...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2284...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2373...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2548...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2467...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2329...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2498...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2397...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2740...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2480...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2427...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2436...  0.1571 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2075...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2317...  0.1591 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2511...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2268...  0.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2025...  0.1580 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2542...  0.1570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2310...  0.1590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2296...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3662...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2533...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2464...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2738...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2304...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2062...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2460...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2360...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2525...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2411...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2349...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2423...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2519...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2545...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2242...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2142...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2566...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2542...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2394...  0.1720 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2678...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2379...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2565...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2364...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2586...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2435...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2035...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2143...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2607...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2549...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2644...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2252...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2084...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2460...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2379...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2286...  0.1680 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2393...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2224...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.1990...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2056...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2294...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2181...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.2819...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2346...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2107...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2463...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2101...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2260...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2349...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2349...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2569...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2125...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2769...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2435...  0.1620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2533...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2355...  0.1582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2399...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2539...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2356...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2212...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2752...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2515...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.2866...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2709...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2541...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2397...  0.1620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2502...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2624...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2327...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2407...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2340...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.2921...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2600...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2753...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2106...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2399...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2616...  0.1561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2314...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2314...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.1888...  0.1591 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2347...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2080...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2419...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2043...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2320...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2184...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2376...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2121...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2135...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2046...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2465...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2152...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2290...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2101...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2081...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2173...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2509...  0.1562 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2518...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2052...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2153...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2110...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2399...  0.1620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2234...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2457...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2273...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2171...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2299...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2426...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2479...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2240...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2491...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2249...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2422...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2417...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2313...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2267...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.1939...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2433...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2364...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2458...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2336...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2383...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2018...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.1906...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2234...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2228...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.1943...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2410...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2351...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2172...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.1866...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.1869...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2200...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2563...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2415...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2503...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2324...  0.1610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2666...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2440...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2447...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2470...  0.1560 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2892...  0.1582 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2516...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2267...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2647...  0.1561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2141...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2593...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2373...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2736...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2661...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2311...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2166...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2128...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2429...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2262...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2236...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2335...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2373...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2345...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2055...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2515...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2550...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2432...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2389...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2348...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2299...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2412...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2612...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.2961...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2578...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2335...  0.1581 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2355...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2158...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2663...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2413...  0.1601 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2458...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2183...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2371...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2646...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2250...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2111...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2193...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2366...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2321...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2306...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2202...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2103...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2676...  0.1730 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2247...  0.1590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2342...  0.1600 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2403...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2042...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2170...  0.1571 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2342...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2196...  0.1561 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.1970...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2338...  0.1570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2239...  0.1580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2214...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3572...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2417...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2422...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2590...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2189...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.1920...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2402...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2388...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2477...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2242...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2249...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2312...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2316...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2426...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2138...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2062...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2422...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2566...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2276...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2621...  0.1690 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2304...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2499...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2365...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2556...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2397...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.1855...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2020...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2521...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2415...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2465...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2158...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2039...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2402...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2311...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2237...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2358...  0.1640 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2108...  0.1651 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.1947...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.1949...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2177...  0.1572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2056...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2685...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2268...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2058...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2381...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2074...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2100...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2265...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2347...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2571...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2043...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2766...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2418...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2420...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2251...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2308...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2372...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2315...  0.1561 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2011...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2601...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2413...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2770...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2488...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2365...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2304...  0.1611 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2425...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2511...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2146...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2305...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2224...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2712...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2515...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2671...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2072...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2323...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2471...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2283...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2209...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.1907...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2368...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.1816...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2316...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2032...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2204...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2089...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2341...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2018...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2094...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2000...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2332...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.1965...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2205...  0.1575 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2022...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2026...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2164...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2386...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2332...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.1899...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2139...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.1991...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2352...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2148...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2305...  0.1582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2258...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2233...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2247...  0.1591 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2380...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2329...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2142...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2399...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2114...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2334...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2205...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2172...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2018...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.1857...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2348...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2276...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2306...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2218...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2356...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.2001...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1837...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2193...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2111...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1785...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2214...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2197...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2072...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1835...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1793...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2098...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2460...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2326...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2278...  0.1572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2195...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2611...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2465...  0.1591 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2268...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2183...  0.1620 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2834...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2400...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2173...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2577...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2103...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2541...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2323...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2551...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2561...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2314...  0.1610 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.1983...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.1965...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2515...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2184...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2123...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2263...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2216...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2157...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.1984...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2418...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2406...  0.1582 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2298...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2297...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2257...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2168...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2250...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2441...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2727...  0.1690 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2306...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2353...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2221...  0.1600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2059...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2610...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2250...  0.1601 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2309...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.1933...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2180...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2571...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2144...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.1925...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2031...  0.1640 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2279...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2223...  0.1590 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2134...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2250...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2054...  0.1572 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2577...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2110...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2203...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2247...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2083...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.1963...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2374...  0.1571 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2107...  0.1560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.1824...  0.1570 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2289...  0.1580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2165...  0.1581 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2108...  0.1580 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3273...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2350...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2261...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2511...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2025...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.1880...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2173...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2241...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2358...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2139...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2155...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2207...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2296...  0.1591 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2340...  0.1581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2009...  0.1730 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.1998...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2306...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2406...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2264...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2450...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2278...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2328...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2254...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2478...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2272...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1875...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.1883...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2435...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2442...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2407...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2021...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.1932...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2293...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2237...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.1961...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2343...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2023...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1817...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1838...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2048...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.1961...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2549...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2140...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.1907...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2384...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2006...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2205...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2171...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2153...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2442...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.1981...  0.1582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2566...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2254...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2272...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2145...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2198...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2312...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2173...  0.1591 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2080...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2460...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2317...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2583...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2554...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2284...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2181...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2426...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2482...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2071...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2190...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2094...  0.1720 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2675...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2393...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2587...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2099...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2226...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2359...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2158...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2140...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1753...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2176...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1877...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2216...  0.1591 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.1855...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2090...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.1958...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2070...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2071...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2063...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.1920...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2228...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.1954...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2055...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.1933...  0.1582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.1916...  0.1571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2116...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2328...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2295...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.1882...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.1962...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.1987...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2125...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2006...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2268...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2089...  0.1581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2106...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2020...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2131...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2143...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2052...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2229...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.1936...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2236...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2140...  0.1581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2060...  0.1581 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2012...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1800...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2249...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2144...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2160...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2135...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2147...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.1916...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1739...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2135...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2054...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1797...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2218...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2127...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.1919...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1707...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1717...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2053...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2419...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2249...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2257...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2155...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2467...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2394...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2138...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2288...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2707...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2299...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2172...  0.1582 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2581...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2008...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2425...  0.1571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2331...  0.1610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2497...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2371...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2104...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.1850...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.1836...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2292...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2082...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2096...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2096...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2093...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2047...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.1902...  0.1610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2350...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2393...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2247...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2079...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2134...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2125...  0.1610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2128...  0.1670 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2363...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2717...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2255...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2196...  0.1591 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2190...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.1995...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2476...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2183...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2182...  0.1700 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.1912...  0.1571 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2157...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2529...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.1929...  0.1590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.1965...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2021...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2060...  0.1600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2149...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2011...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2038...  0.1592 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.1993...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2470...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2036...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2007...  0.1610 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2132...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.1799...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.1964...  0.1671 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2193...  0.1580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2000...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1740...  0.1570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2225...  0.1570 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2094...  0.1560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2048...  0.1570 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3960_l512.ckpt\n",
      "Farther.\n",
      "\n",
      "\"Anna will the work in the wagater and he pursuishly. It sought it, if\n",
      "they've not seen it out.\"\n",
      "\n",
      "\"Oh, then, inded that,\" he siin silent.\n",
      "\n",
      "\"Why, these death! Well, that, it's only in the carrying it in the stail,\"\n",
      " the position, so shouting herself, and was simpining tomernow and agond\n",
      "torance.\n",
      "\n",
      "\"I should not be often anyone. A mind of a tron, I chanced howe it all in\n",
      "aristoca!\" said Anna, stating his bitter with which. \"I teing this woman\n",
      "is anxieticed in tomorrow is how they'll greater the sights, and I can\n",
      "not live to me, I could not to go out, I'll consincurate them what it\n",
      "here about them that I am,\" he said to Anna with in smill, said that she\n",
      "would nut he said when he coule obviously at him, and askidiously apparant\n",
      "from the ball to see he how all this teress it was not lef in her\n",
      "same, and he did not, becaue Anna, when a good might said nothing, andwhere\n",
      "was an old man, and that in the most secondred how, if it could not\n",
      "see herself that. She shamened them, and she shoothused Vronsky, and thought\n",
      "of this self-consciousness had not atrectly and marrilge. The mealorsy,\n",
      "thought on hurd by the strangeres out of the common, he won'd go and\n",
      "married, and thanked that this in pasting. A sone of telling them\n",
      "on the peace with the counting, with the peoss, and as the powers will hat\n",
      "sever positively into a luttar, she caused me fortony at the meads. Besidess\n",
      "was sent on the least of the sky sincery who had bay so too, talking\n",
      "of more than ever all he had come to her attanting to her,\n",
      "and a subject was in sense of talism, and, simply for the prince-as the\n",
      "consideration of his coalst to method, and a little baby was that all\n",
      "his sake, this were still married with them.\n",
      "\n",
      "The carriages would can't be astersone to his work, and, went the sone\n",
      "of teess, that the most so as in the cares, and how so as to her how\n",
      "treated herself and the clook or hardy, however another mill are the\n",
      "conversation of his words to so straight to the passion and the\n",
      "weardy in the meaning.\n",
      "\n",
      "\"I\n"
     ]
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

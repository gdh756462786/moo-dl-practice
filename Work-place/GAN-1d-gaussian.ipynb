{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prapare packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# true data distribution: p_d(x)\n",
    "class DataDistribution(object):\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def sample(self, N):\n",
    "        samples = np.random.normal(self.mu, self.sigma, N)\n",
    "        #samples.sort()\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p_z(z)\n",
    "class NoiseDistribution(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "\n",
    "    # equally spaced samples + noise\n",
    "    def sample(self, N):\n",
    "        offset = np.random.random(N) * (float(self.range) / N)\n",
    "        samples = np.linspace(-self.range, self.range, N) + offset\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define layers to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network layer that we will use for discriminator - D\n",
    "def discriminator_layer(inputs, n_hidden=32):\n",
    "    w_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    \n",
    "    # 1st layer\n",
    "    w0 = tf.get_variable(\"w0\", [inputs.get_shape()[1], n_hidden], initializer=w_initializer)\n",
    "    b0 = tf.get_variable(\"b0\", [n_hidden], initializer=tf.constant_initializer(0.0))\n",
    "    l0 = tf.nn.relu(tf.matmul(inputs, w0) + b0)\n",
    "    \n",
    "    # 2nd layer\n",
    "    w1 = tf.get_variable(\"w1\", [n_hidden, n_hidden], initializer=w_initializer)\n",
    "    b1 = tf.get_variable(\"b1\", [n_hidden], initializer=tf.constant_initializer(0.0))\n",
    "    l1 = tf.nn.relu(tf.matmul(l0, w1) + b1)\n",
    "    \n",
    "    # output layer\n",
    "    w2 = tf.get_variable(\"w2\", [n_hidden, 1], initializer=w_initializer)\n",
    "    b2 = tf.get_variable(\"b2\", [1], initializer=tf.constant_initializer(0.0))\n",
    "    lo = tf.matmul(l1, w2) + b2\n",
    "    \n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network layer that we will use for generator - G\n",
    "def generator_layer(inputs, n_hidden=32):\n",
    "    w_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    \n",
    "    # 1st layer\n",
    "    w0 = tf.get_variable(\"w0\", [inputs.get_shape()[1], n_hidden], initializer=w_initializer)\n",
    "    b0 = tf.get_variable(\"b0\", [n_hidden], initializer=tf.constant_initializer(0.0))\n",
    "    l0 = tf.nn.relu(tf.matmul(inputs, w0) + b0)\n",
    "    \n",
    "    # 2nd layer\n",
    "    w1 = tf.get_variable(\"w1\", [n_hidden, n_hidden], initializer=w_initializer)\n",
    "    b1 = tf.get_variable(\"b1\", [n_hidden], initializer=tf.constant_initializer(0.0))\n",
    "    l1 = tf.nn.relu(tf.matmul(l0, w1) + b1)\n",
    "    \n",
    "    # output layer\n",
    "    w2 = tf.get_variable(\"w2\", [n_hidden, 1], initializer=w_initializer)\n",
    "    b2 = tf.get_variable(\"b2\", [1], initializer=tf.constant_initializer(0.0))\n",
    "    lo = tf.sigmoid( tf.matmul(l1, w2) + b2 )\n",
    "    \n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(loss, var_list, num_decay_steps=400, initial_learning_rate=0.03):\n",
    "    decay = 0.95\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        initial_learning_rate,\n",
    "        batch,\n",
    "        num_decay_steps,\n",
    "        decay,\n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step=batch,\n",
    "        var_list=var_list\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of samples to draw\n",
    "N = 1000\n",
    "\n",
    "# number of bins when computing histogram\n",
    "n_bins = N//10\n",
    "\n",
    "# mu & sigma for nomal distribution\n",
    "mu, sigma = 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at sample distribution a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuEtSgwx3SWrQ/wGX2ch8J52ZsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a053d00ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the probability distribution(gaussian) for discriminator D's input\n",
    "# draw N sample of p_d\n",
    "p_d = DataDistribution(mu, sigma)\n",
    "p_d_sample = p_d.sample(N)\n",
    "plot_hist, _, __ = plt.hist(p_d_sample, bins=n_bins, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a054e99390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the probability distribution(uniform) for generator G's input\n",
    "data_range = 5\n",
    "p_z = NoiseDistribution(data_range)\n",
    "p_z_sample = p_z.sample(N)\n",
    "plot_hist, _, __ = plt.hist(p_z_sample, bins=n_bins, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre training graph\n",
    "with tf.variable_scope('D-pre'):\n",
    "    inputs_pre = tf.placeholder(tf.float32, shape=[None, 1], name='inputs')\n",
    "    targets_pre = tf.placeholder(tf.float32, shape=[None, 1], name='targets')\n",
    "    D_pre = discriminator_layer(inputs_pre)\n",
    "    \n",
    "    # loss\n",
    "    loss_pre = tf.reduce_mean(tf.square(D_pre - targets_pre))\n",
    "    \n",
    "    # optimizer\n",
    "    t_vars = tf.trainable_variables()\n",
    "    optimizer_pre = optimizer(loss_pre, t_vars, 400, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('G'):\n",
    "    z = tf.placeholder(tf.float32, shape=[None, 1], name='inputs')\n",
    "    G = generator_layer(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('D') as scope:\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 1], name='inputs')\n",
    "    D_real = discriminator_layer(x)\n",
    "    scope.reuse_variables()\n",
    "    D_fake = discriminator_layer(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-2  # to prevent log(0) case\n",
    "loss_G = tf.reduce_mean(-tf.log(D_fake + eps))\n",
    "loss_D = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))\n",
    "\n",
    "params_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='G')\n",
    "params_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D')\n",
    "\n",
    "optimizer_G = optimizer(loss_G, params_G, 400, learning_rate * 0.5)\n",
    "optimizer_D = optimizer(loss_D, params_D, 400, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train decision surface\n",
    "* If decider is reasonably accurate to start, we get much faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_pre_train = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize data distribution\n",
    "def normalize_data(hist_density, edges):\n",
    "    x = edges[1:]\n",
    "    \n",
    "    # will make [0 ~ 1]\n",
    "    max_hist = np.max(hist_density)\n",
    "    min_hist = np.min(hist_density)\n",
    "    normed_y = (hist_density - min_hist) / (max_hist - min_hist)\n",
    "    \n",
    "    return x, normed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open session and initialize all variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training :  0/1000, cost: 0.1709\n",
      "pre-training :  10/1000, cost: 0.1396\n",
      "pre-training :  20/1000, cost: 0.1087\n",
      "pre-training :  30/1000, cost: 0.0985\n",
      "pre-training :  40/1000, cost: 0.0780\n",
      "pre-training :  50/1000, cost: 0.0996\n",
      "pre-training :  60/1000, cost: 0.0982\n",
      "pre-training :  70/1000, cost: 0.1034\n",
      "pre-training :  80/1000, cost: 0.0898\n",
      "pre-training :  90/1000, cost: 0.0782\n",
      "pre-training :  100/1000, cost: 0.0827\n",
      "pre-training :  110/1000, cost: 0.0649\n",
      "pre-training :  120/1000, cost: 0.0820\n",
      "pre-training :  130/1000, cost: 0.0974\n",
      "pre-training :  140/1000, cost: 0.1006\n",
      "pre-training :  150/1000, cost: 0.0795\n",
      "pre-training :  160/1000, cost: 0.0859\n",
      "pre-training :  170/1000, cost: 0.0936\n",
      "pre-training :  180/1000, cost: 0.0828\n",
      "pre-training :  190/1000, cost: 0.0886\n",
      "pre-training :  200/1000, cost: 0.0611\n",
      "pre-training :  210/1000, cost: 0.0740\n",
      "pre-training :  220/1000, cost: 0.0782\n",
      "pre-training :  230/1000, cost: 0.0714\n",
      "pre-training :  240/1000, cost: 0.0947\n",
      "pre-training :  250/1000, cost: 0.0895\n",
      "pre-training :  260/1000, cost: 0.0617\n",
      "pre-training :  270/1000, cost: 0.0829\n",
      "pre-training :  280/1000, cost: 0.0663\n",
      "pre-training :  290/1000, cost: 0.0615\n",
      "pre-training :  300/1000, cost: 0.0633\n",
      "pre-training :  310/1000, cost: 0.0715\n",
      "pre-training :  320/1000, cost: 0.0991\n",
      "pre-training :  330/1000, cost: 0.0809\n",
      "pre-training :  340/1000, cost: 0.1073\n",
      "pre-training :  350/1000, cost: 0.0855\n",
      "pre-training :  360/1000, cost: 0.0957\n",
      "pre-training :  370/1000, cost: 0.0662\n",
      "pre-training :  380/1000, cost: 0.0637\n",
      "pre-training :  390/1000, cost: 0.0803\n",
      "pre-training :  400/1000, cost: 0.0852\n",
      "pre-training :  410/1000, cost: 0.0878\n",
      "pre-training :  420/1000, cost: 0.0718\n",
      "pre-training :  430/1000, cost: 0.0732\n",
      "pre-training :  440/1000, cost: 0.0959\n",
      "pre-training :  450/1000, cost: 0.0765\n",
      "pre-training :  460/1000, cost: 0.0560\n",
      "pre-training :  470/1000, cost: 0.0791\n",
      "pre-training :  480/1000, cost: 0.0664\n",
      "pre-training :  490/1000, cost: 0.0874\n",
      "pre-training :  500/1000, cost: 0.0703\n",
      "pre-training :  510/1000, cost: 0.0944\n",
      "pre-training :  520/1000, cost: 0.0617\n",
      "pre-training :  530/1000, cost: 0.0857\n",
      "pre-training :  540/1000, cost: 0.0614\n",
      "pre-training :  550/1000, cost: 0.0644\n",
      "pre-training :  560/1000, cost: 0.0822\n",
      "pre-training :  570/1000, cost: 0.0801\n",
      "pre-training :  580/1000, cost: 0.0884\n",
      "pre-training :  590/1000, cost: 0.0883\n",
      "pre-training :  600/1000, cost: 0.0773\n",
      "pre-training :  610/1000, cost: 0.0812\n",
      "pre-training :  620/1000, cost: 0.0583\n",
      "pre-training :  630/1000, cost: 0.0653\n",
      "pre-training :  640/1000, cost: 0.0810\n",
      "pre-training :  650/1000, cost: 0.0815\n",
      "pre-training :  660/1000, cost: 0.0758\n",
      "pre-training :  670/1000, cost: 0.0572\n",
      "pre-training :  680/1000, cost: 0.0779\n",
      "pre-training :  690/1000, cost: 0.0551\n",
      "pre-training :  700/1000, cost: 0.0712\n",
      "pre-training :  710/1000, cost: 0.0700\n",
      "pre-training :  720/1000, cost: 0.0785\n",
      "pre-training :  730/1000, cost: 0.0923\n",
      "pre-training :  740/1000, cost: 0.0773\n",
      "pre-training :  750/1000, cost: 0.0770\n",
      "pre-training :  760/1000, cost: 0.0911\n",
      "pre-training :  770/1000, cost: 0.0733\n",
      "pre-training :  780/1000, cost: 0.0695\n",
      "pre-training :  790/1000, cost: 0.0818\n",
      "pre-training :  800/1000, cost: 0.0782\n",
      "pre-training :  810/1000, cost: 0.0927\n",
      "pre-training :  820/1000, cost: 0.0764\n",
      "pre-training :  830/1000, cost: 0.0791\n",
      "pre-training :  840/1000, cost: 0.0741\n",
      "pre-training :  850/1000, cost: 0.1026\n",
      "pre-training :  860/1000, cost: 0.0800\n",
      "pre-training :  870/1000, cost: 0.0647\n",
      "pre-training :  880/1000, cost: 0.0868\n",
      "pre-training :  890/1000, cost: 0.1051\n",
      "pre-training :  900/1000, cost: 0.0945\n",
      "pre-training :  910/1000, cost: 0.0769\n",
      "pre-training :  920/1000, cost: 0.0975\n",
      "pre-training :  930/1000, cost: 0.0825\n",
      "pre-training :  940/1000, cost: 0.0801\n",
      "pre-training :  950/1000, cost: 0.0928\n",
      "pre-training :  960/1000, cost: 0.0588\n",
      "pre-training :  970/1000, cost: 0.0746\n",
      "pre-training :  980/1000, cost: 0.0671\n",
      "pre-training :  990/1000, cost: 0.0662\n",
      "pre-training finished!\n"
     ]
    }
   ],
   "source": [
    "# pre-training discriminator\n",
    "for i in range(n_pre_train):\n",
    "    drawn_samples = p_d.sample(N)\n",
    "    hist_density, edges = np.histogram(drawn_samples, bins=n_bins, density=True)\n",
    "    x, y = normalize_data(hist_density, edges)\n",
    "\n",
    "    # Execute one training step\n",
    "    fd = {inputs_pre: np.reshape(x, (n_bins, 1)), \n",
    "          targets_pre: np.reshape(y, (n_bins, 1))}\n",
    "    loss_out, _ = sess.run([loss_pre, optimizer_pre], feed_dict=fd)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print('pre-training :  {}/{}, cost: {:.4f}'.format(i, n_pre_train, loss_out))\n",
    "print('pre-training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store pre-trained variables\n",
    "params_pre = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D-pre')\n",
    "weights_D_pre = sess.run(params_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = 3000\n",
    "\n",
    "# batch size\n",
    "M = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-091648e011d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_D_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_D_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# copy weights from pre-training over to new D network\n",
    "tf.global_variables_initializer().run()\n",
    "print(len(params_D))\n",
    "print(len(params_G))\n",
    "print(len(weights_D_pre))\n",
    "for i, v in enumerate(params_D):\n",
    "    sess.run(v.assign(weights_D_pre[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training-loop\n",
    "for i in range(n_train):\n",
    "    np.random.seed(np.random.randint(0, n_train))\n",
    "\n",
    "    # update discriminator\n",
    "    x_ = p_d.sample(M)\n",
    "    z_ = p_z.sample(M)\n",
    "    \n",
    "    fd_D = {x: np.reshape(x_, (M, 1)), z: np.reshape(z_, (M, 1))}\n",
    "    loss_out_D, _ = sess.run([loss_D, optimizer_D], feed_dict=fd_D)\n",
    "\n",
    "    # update generator\n",
    "    z_ = p_z.sample(M)\n",
    "    fd_G = {z: np.reshape(z_, (B, 1))}\n",
    "    loss_out_G, _ = sess.run([loss_G, optimizer_G], feed_dict=fd_G)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print('[{}/{}]: loss_d : {:.3f}, loss_g : {.3f}'.format(i, n_train, loss_out_D, loss_out_G))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
